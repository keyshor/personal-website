<!DOCTYPE html>
<html>
    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Projects - Kishor Jothimurugan</title><link rel="apple-touch-icon" sizes="180x180" href=https://seas.upenn.edu/~kishor/apple-touch-icon.png>
	<link rel="icon" type="image/png" sizes="32x32" href=https://seas.upenn.edu/~kishor/favicon-32x32.png>
	<link rel="icon" type="image/png" sizes="16x16" href=https://seas.upenn.edu/~kishor/favicon-16x16.png>
	<link rel="manifest" href=https://seas.upenn.edu/~kishor/site.webmanifest>
	<link rel="mask-icon" href=https://seas.upenn.edu/~kishor/safari-pinned-tab.svg color="#5bbad5">
	<meta name="msapplication-TileColor" content="#da532c">
	<meta name="theme-color" content="#ffffff">

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Projects" />
<meta property="og:description" content="Projects  Formal Methods for Reinforcement Learning. Classical reinforcement learning (RL) involves learning a controller policy that maximizes a reward function (in expectation) under unknown system dynamics. This line of work focusses on applying techniques from logic and formal methods to reinforcement learning. An example of this approach is our work on programmable rewards for RL. We designed a logical specification language for specifying the expected behavior of the system." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://seas.upenn.edu/~kishor/projects/" />
<meta property="article:published_time" content="2019-10-11T10:40:07-04:00" />
<meta property="article:modified_time" content="2019-10-11T10:40:07-04:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Projects"/>
<meta name="twitter:description" content="Projects  Formal Methods for Reinforcement Learning. Classical reinforcement learning (RL) involves learning a controller policy that maximizes a reward function (in expectation) under unknown system dynamics. This line of work focusses on applying techniques from logic and formal methods to reinforcement learning. An example of this approach is our work on programmable rewards for RL. We designed a logical specification language for specifying the expected behavior of the system."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href=https://seas.upenn.edu/~kishor/css/normalize.css />
	<link rel="stylesheet" type="text/css" media="screen" href=https://seas.upenn.edu/~kishor/css/main.css/><script src=https://seas.upenn.edu/~kishor/js/feather.min.js></script><script src=https://seas.upenn.edu/~kishor/js/main.js></script>
</head>

    <body>
        <div id="content">





<div class="header wrapper">
  <span class="page_title">
    Projects
  </span>
  <a href="https://seas.upenn.edu/~kishor" title="Home" class="page_meta">Return Home</a>
</div>


<div class="content wrapper">
  <h3 id="projects">Projects</h3>

<ul>
<li><p><strong>Formal Methods for Reinforcement Learning.</strong>
Classical reinforcement learning (RL) involves learning a controller policy that maximizes a reward function (in expectation) under unknown system dynamics. This line of work focusses on applying techniques from logic and formal methods to reinforcement learning. An example of this approach is our work on programmable rewards for RL. We designed a
logical specification language for specifying the expected behavior of the system. Specifications in this language are then
compiled to reward functions which can be used in RL algorithms (along with other things necessary for learning such as the neural network representation of the policy). Our current work focusses on improving the efficiency of RL methods
and also ways to verify learned policies.</p></li>

<li><p><strong>Streaming Algorithms over Probabilistic Streams.</strong>
In streaming algorithms, streams are defined to be sequences of data points. In this project, we consider probabilistic streams which are sequences of distributions over a finite set of events. Each probabilistic stream can be interpreted as a distribution over regular streams. The goal is then to compute the expected value of a given function (that assigns values to regular streams) on an input probabilistic stream. We look at automata based functions and analyze the space complexity of such computations in the streaming setting.</p></li>
</ul>

</div>

</div>
        <div class="footer wrapper">
	<object align="left">Last Updated October 11, 2019</object>
	<nav class="nav">
		<div><a href="https://github.com/hadisinaee/avicenna">Avicenna Theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-1234-6', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>
	feather.replace()

</script>
    </body>
</html>
